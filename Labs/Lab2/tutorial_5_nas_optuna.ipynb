{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Neural Architecture Search (NAS) with Mase and Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll see how Mase can be integrated with Optuna, the popular hyperparameter optimization framework, to search for a Bert model optimized for sequence classification on the IMDb dataset. We'll take the Optuna-generated model and import it into Mase, then run the CompressionPipeline to prepare the model for edge deployment by quantizing and pruning its weights.\n",
    "\n",
    "As we'll see, running Architecture Search with Mase/Optuna involves the following steps.\n",
    "\n",
    "1. **Define the search space**: this is a dictionary containing the range of values for each parameter at each layer in the model.\n",
    "\n",
    "2. **Write the model constructor**: this is a function which uses Optuna utilities to sample a model from the search space, and constructs the model using transformers from_config class method.\n",
    "\n",
    "3. **Write the objective function**: this function calls on the model constructor defined in Step 2 and defines the training/evaluation setup for each search iteration.\n",
    "\n",
    "4. **Go!** Choose an Optuna sampler, create a study and launch the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fetch the dataset using the `get_tokenized_dataset` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.tools import get_tokenized_dataset\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining the Search Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by defining a search space, i.e. enumerating the possible combinations of hyperparameters that Optuna can choose during search. We'll explore the following range of values for the model's hidden size, intermediate size, number of layers and number of heads, inspired by the [NAS-BERT paper](https://arxiv.org/abs/2105.14444)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from chop.nn.modules import Identity\n",
    "\n",
    "search_space = {\n",
    "    \"num_layers\": [2, 4, 8],\n",
    "    \"num_heads\": [2, 4, 8, 16],\n",
    "    \"hidden_size\": [128, 192, 256, 384, 512],\n",
    "    \"intermediate_size\": [512, 768, 1024, 1536, 2048],\n",
    "    \"linear_layer_choices\": [\n",
    "        nn.Linear,\n",
    "        Identity,\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a Model Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following function, which will get called in each iteration of the search process. The function is passed the `trial` argument, which is an Optuna object that comes with many functionalities - see the [Trial documentation](https://optuna.readthedocs.io/en/stable/reference/trial.html) for more details. Here, we use the `trial.suggest_int` and `trial.suggest_categorical` functions to trigger the chosen sampler to choose parameter choices and layer types. The suggested integer is the index into the search space for each parameter, which we defined in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "\n",
    "\n",
    "def construct_model(trial):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Update the paramaters in the config\n",
    "    for param in [\n",
    "        \"num_layers\",\n",
    "        \"num_heads\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\",\n",
    "    ]:\n",
    "        chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n",
    "        setattr(config, param, search_space[param][chosen_idx])\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == nn.Linear:\n",
    "                continue\n",
    "            elif new_layer_cls == Identity:\n",
    "                new_layer = Identity()\n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {new_layer_cls}\")\n",
    "\n",
    "    return trial_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the objective function for the search, which gets called on each trial. In each trial, we create a new model instace with chosen hyperparameters according to the defined sampler. We then use the `get_trainer` utility in Mase to run a training loop on the IMDb dataset for a number of epochs. Finally, we use `evaluate` to report back the classification accuracy on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chop.tools import get_trainer\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Define the model\n",
    "    model = construct_model(trial)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Set the model as an attribute so we can fetch it later\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launching the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna provides a number of samplers, for example:\n",
    "\n",
    "* **GridSampler**: iterates through every possible combination of hyperparameters in the search space\n",
    "* **RandomSampler**: chooses a random combination of hyperparameters in each iteration\n",
    "* **TPESampler**: uses Tree-structured Parzen Estimator algorithm to choose hyperparameter values.\n",
    "\n",
    "You can define the chosen sampler by simply importing from `optuna.samplers` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "sampler = RandomSampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the pieces in place, we can launch the search as follows. The number of trials is set to 1 so you can go get a coffee for 10 minutes, then proceed with the tutorial. However, this will essentially be a random model - for better results, set this to 100 and leave it running overnight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-05 00:31:11,311] A new study created in memory with name: bert-tiny-nas-study\n",
      "/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.nn.modules.linear.Linear'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/optuna/distributions.py:515: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'chop.nn.modules.identity.Identity'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/rds/general/user/jj21/home/adlsystems/mase/src/chop/tools/huggingface.py:157: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.439200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.370700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.367800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-05 00:32:30,837] Trial 0 finished with value: 0.84788 and parameters: {'num_layers': 1, 'num_heads': 1, 'hidden_size': 4, 'intermediate_size': 0, 'bert.encoder.layer.0.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.0.intermediate.dense_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.0.output.dense_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.intermediate.dense_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'torch.nn.modules.linear.Linear'>}. Best is trial 0 with value: 0.84788.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=1,\n",
    "    timeout=60 * 60 * 24,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the model associated with the best trial as follows, and export to be used in future tutorials. In Tutorial 6, we'll see how to run mixed-precision quantization search on top of the model we've just found through NAS to further find the optimal quantization mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "\n",
    "model = study.best_trial.user_attrs[\"model\"].cpu()\n",
    "\n",
    "# with open(f\"{Path.home()}/adlsystems/tutorial_5_best_model.pkl\", \"wb\") as f:\n",
    "#     dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the Optimized Model with CompressionPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the CompressionPipeline in Mase to run uniform quantization and pruning over the searched model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`past_key_values` were not specified as input names, but model.config.use_cache = True. Setting model.config.use_cache = False.\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mGetting dummy input for prajjwal1/bert-tiny.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "tensor([[ 101, 9932, 2089, 2202, 2058, 1996, 2088, 2028, 2154,  102],\n",
      "        [ 101, 2023, 2003, 2339, 2017, 2323, 4553, 4748, 4877,  102]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "tensor([[[-0.0563, -2.1479,  0.9149,  ..., -0.1527, -0.5602,  1.4918],\n",
      "         [-0.0316, -0.5712,  0.4234,  ..., -1.1844, -0.3044,  0.9887],\n",
      "         [-0.7347, -0.4508,  1.3795,  ..., -1.3949, -1.8731,  0.6145],\n",
      "         ...,\n",
      "         [ 0.7337, -0.5086, -0.3855,  ...,  0.4048, -1.1635,  1.5633],\n",
      "         [-0.4265, -1.3649, -0.9071,  ...,  0.8776, -1.3722,  0.2098],\n",
      "         [-0.4616, -0.7873,  0.4645,  ..., -0.7219, -0.9503,  0.0068]],\n",
      "\n",
      "        [[-0.0563, -2.1479,  0.9149,  ..., -0.1527, -0.5602,  1.4918],\n",
      "         [-0.2146,  0.2368,  0.9101,  ..., -0.2310,  0.7356,  0.9070],\n",
      "         [ 0.5375, -1.4821,  1.2080,  ..., -0.4145, -1.3056,  3.2342],\n",
      "         ...,\n",
      "         [ 0.4696,  0.3898,  0.7087,  ...,  1.0850,  0.3990,  0.4015],\n",
      "         [ 1.0661, -1.3273, -1.3211,  ..., -0.7212,  0.8344,  0.5444],\n",
      "         [-0.4616, -0.7873,  0.4645,  ..., -0.7219, -0.9503,  0.0068]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[ 0.3481, -0.0910,  1.9176,  ...,  0.8788, -0.3995,  0.6323],\n",
      "         [ 0.9181, -0.0434,  0.9408,  ...,  0.1285, -0.3616,  0.2046],\n",
      "         [ 0.4279, -0.6533,  1.1446,  ...,  1.0748, -0.3291,  0.4908],\n",
      "         ...,\n",
      "         [ 0.5235,  0.6055,  0.7444,  ...,  0.3456, -0.8075,  0.1265],\n",
      "         [ 0.3645, -0.3348,  1.4839,  ...,  0.7046,  0.0816, -0.0712],\n",
      "         [ 0.6673, -0.4498,  0.6473,  ...,  0.9796, -0.4107,  0.8167]],\n",
      "\n",
      "        [[ 0.3481, -0.0910,  1.9176,  ...,  0.8788, -0.3995,  0.6323],\n",
      "         [ 0.2766,  0.0715,  1.3448,  ...,  0.1119, -0.7400,  0.8315],\n",
      "         [ 0.3066, -0.4200,  0.8349,  ...,  0.7518, -0.6923,  0.5361],\n",
      "         ...,\n",
      "         [ 0.7826,  0.7087,  0.4668,  ...,  0.6002, -0.5525,  0.1316],\n",
      "         [ 0.5951, -0.5589,  1.6458,  ...,  0.4471,  0.0070,  0.4676],\n",
      "         [ 0.6673, -0.4498,  0.6473,  ...,  0.9796, -0.4107,  0.8167]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.3481, -0.0910,  1.9176,  ...,  0.8788, -0.3995,  0.6323],\n",
      "         [ 0.9181, -0.0434,  0.9408,  ...,  0.1285, -0.3616,  0.2046],\n",
      "         [ 0.4279, -0.6533,  1.1446,  ...,  1.0748, -0.3291,  0.4908],\n",
      "         ...,\n",
      "         [ 0.5235,  0.6055,  0.7444,  ...,  0.3456, -0.8075,  0.1265],\n",
      "         [ 0.3645, -0.3348,  1.4839,  ...,  0.7046,  0.0816, -0.0712],\n",
      "         [ 0.6673, -0.4498,  0.6473,  ...,  0.9796, -0.4107,  0.8167]],\n",
      "\n",
      "        [[ 0.3481, -0.0910,  1.9176,  ...,  0.8788, -0.3995,  0.6323],\n",
      "         [ 0.2766,  0.0715,  1.3448,  ...,  0.1119, -0.7400,  0.8315],\n",
      "         [ 0.3066, -0.4200,  0.8349,  ...,  0.7518, -0.6923,  0.5361],\n",
      "         ...,\n",
      "         [ 0.7826,  0.7087,  0.4668,  ...,  0.6002, -0.5525,  0.1316],\n",
      "         [ 0.5951, -0.5589,  1.6458,  ...,  0.4471,  0.0070,  0.4676],\n",
      "         [ 0.6673, -0.4498,  0.6473,  ...,  0.9796, -0.4107,  0.8167]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.3481, -0.0910,  1.9176,  ...,  0.4706, -0.9177,  0.4501],\n",
      "          [ 0.5003,  0.6168,  1.5445,  ...,  0.8788, -0.3995,  0.6323]],\n",
      "\n",
      "         [[ 0.9181, -0.0434,  0.9408,  ..., -0.2720, -0.0282,  0.7819],\n",
      "          [ 0.7626,  0.7887,  0.9844,  ...,  0.1285, -0.3616,  0.2046]],\n",
      "\n",
      "         [[ 0.4279, -0.6533,  1.1446,  ...,  0.2634, -0.3197,  0.2579],\n",
      "          [ 0.3870,  0.2905,  0.6351,  ...,  1.0748, -0.3291,  0.4908]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5235,  0.6055,  0.7444,  ..., -0.4876, -0.8365,  0.6808],\n",
      "          [ 0.2498,  0.3404,  1.1546,  ...,  0.3456, -0.8075,  0.1265]],\n",
      "\n",
      "         [[ 0.3645, -0.3348,  1.4839,  ...,  0.0608, -0.4071,  0.6945],\n",
      "          [ 0.6424,  0.8735,  0.4329,  ...,  0.7046,  0.0816, -0.0712]],\n",
      "\n",
      "         [[ 0.6673, -0.4498,  0.6473,  ...,  0.2733, -0.4342,  1.0430],\n",
      "          [ 0.3566,  0.5280,  0.4080,  ...,  0.9796, -0.4107,  0.8167]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3481, -0.0910,  1.9176,  ...,  0.4706, -0.9177,  0.4501],\n",
      "          [ 0.5003,  0.6168,  1.5445,  ...,  0.8788, -0.3995,  0.6323]],\n",
      "\n",
      "         [[ 0.2766,  0.0715,  1.3448,  ..., -0.4686,  0.2094,  0.5004],\n",
      "          [ 0.5940,  1.3530,  0.4722,  ...,  0.1119, -0.7400,  0.8315]],\n",
      "\n",
      "         [[ 0.3066, -0.4200,  0.8349,  ...,  0.1813,  0.0684,  0.5655],\n",
      "          [ 0.5588,  0.7091,  0.3504,  ...,  0.7518, -0.6923,  0.5361]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7826,  0.7087,  0.4668,  ...,  0.1688, -0.5303,  0.6467],\n",
      "          [ 0.5467,  0.3297,  0.7499,  ...,  0.6002, -0.5525,  0.1316]],\n",
      "\n",
      "         [[ 0.5951, -0.5589,  1.6458,  ..., -0.3005, -0.5258,  0.5025],\n",
      "          [ 1.0658,  1.2103,  0.3450,  ...,  0.4471,  0.0070,  0.4676]],\n",
      "\n",
      "         [[ 0.6673, -0.4498,  0.6473,  ...,  0.2733, -0.4342,  1.0430],\n",
      "          [ 0.3566,  0.5280,  0.4080,  ...,  0.9796, -0.4107,  0.8167]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.5014,  0.1413,  0.1415,  ..., -0.9704,  0.7777,  0.7969],\n",
      "         [ 0.6076,  0.3658, -0.2159,  ...,  0.0230,  0.5557,  0.5109],\n",
      "         [-0.0263,  0.1063,  0.1257,  ...,  0.7453,  0.1507,  0.0996],\n",
      "         ...,\n",
      "         [ 0.0587,  0.5441,  0.4012,  ..., -0.3599, -0.1997,  0.0344],\n",
      "         [ 0.2227,  0.3138,  0.4763,  ...,  1.0023,  0.1026,  0.0584],\n",
      "         [ 0.2001,  0.3239,  0.3363,  ...,  0.3579,  0.2866,  0.1008]],\n",
      "\n",
      "        [[ 0.5014,  0.1413,  0.1415,  ..., -0.9704,  0.7777,  0.7969],\n",
      "         [-0.0412,  0.3636,  0.0537,  ..., -0.2164,  0.1394, -0.0590],\n",
      "         [ 0.9760,  0.5155, -0.5424,  ..., -0.0954,  0.2611,  0.3520],\n",
      "         ...,\n",
      "         [-0.2473,  0.3493, -0.0972,  ...,  0.0012,  0.5885, -0.3886],\n",
      "         [ 0.4681,  0.2857,  0.1056,  ...,  0.9310,  0.1864, -0.3146],\n",
      "         [ 0.2001,  0.3239,  0.3363,  ...,  0.3579,  0.2866,  0.1008]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.5014,  0.1413,  0.1415,  ..., -0.9704,  0.7777,  0.7969],\n",
      "         [ 0.6076,  0.3658, -0.2159,  ...,  0.0230,  0.5557,  0.5109],\n",
      "         [-0.0263,  0.1063,  0.1257,  ...,  0.7453,  0.1507,  0.0996],\n",
      "         ...,\n",
      "         [ 0.0587,  0.5441,  0.4012,  ..., -0.3599, -0.1997,  0.0344],\n",
      "         [ 0.2227,  0.3138,  0.4763,  ...,  1.0023,  0.1026,  0.0584],\n",
      "         [ 0.2001,  0.3239,  0.3363,  ...,  0.3579,  0.2866,  0.1008]],\n",
      "\n",
      "        [[ 0.5014,  0.1413,  0.1415,  ..., -0.9704,  0.7777,  0.7969],\n",
      "         [-0.0412,  0.3636,  0.0537,  ..., -0.2164,  0.1394, -0.0590],\n",
      "         [ 0.9760,  0.5155, -0.5424,  ..., -0.0954,  0.2611,  0.3520],\n",
      "         ...,\n",
      "         [-0.2473,  0.3493, -0.0972,  ...,  0.0012,  0.5885, -0.3886],\n",
      "         [ 0.4681,  0.2857,  0.1056,  ...,  0.9310,  0.1864, -0.3146],\n",
      "         [ 0.2001,  0.3239,  0.3363,  ...,  0.3579,  0.2866,  0.1008]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.5014,  0.1413,  0.1415,  ..., -0.1176,  0.7704, -0.0852],\n",
      "          [ 0.3380,  0.4168,  0.0567,  ..., -0.9704,  0.7777,  0.7969]],\n",
      "\n",
      "         [[ 0.6076,  0.3658, -0.2159,  ..., -0.8200,  0.8730, -0.3406],\n",
      "          [ 0.7290,  0.3047,  0.3581,  ...,  0.0230,  0.5557,  0.5109]],\n",
      "\n",
      "         [[-0.0263,  0.1063,  0.1257,  ..., -0.0856,  0.2830, -0.7700],\n",
      "          [ 0.2943,  0.4176,  0.2260,  ...,  0.7453,  0.1507,  0.0996]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0587,  0.5441,  0.4012,  ..., -0.6823,  0.6484, -0.2428],\n",
      "          [ 0.0715, -0.1969, -0.3955,  ..., -0.3599, -0.1997,  0.0344]],\n",
      "\n",
      "         [[ 0.2227,  0.3138,  0.4763,  ..., -0.3141,  0.4449,  0.0512],\n",
      "          [ 0.9529, -0.1069,  0.0972,  ...,  1.0023,  0.1026,  0.0584]],\n",
      "\n",
      "         [[ 0.2001,  0.3239,  0.3363,  ..., -0.6182,  0.1649, -1.0363],\n",
      "          [ 0.6499,  0.2176, -0.5196,  ...,  0.3579,  0.2866,  0.1008]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5014,  0.1413,  0.1415,  ..., -0.1176,  0.7704, -0.0852],\n",
      "          [ 0.3380,  0.4168,  0.0567,  ..., -0.9704,  0.7777,  0.7969]],\n",
      "\n",
      "         [[-0.0412,  0.3636,  0.0537,  ..., -1.0787,  1.0539, -0.1846],\n",
      "          [ 0.2554, -0.7264,  0.1086,  ..., -0.2164,  0.1394, -0.0590]],\n",
      "\n",
      "         [[ 0.9760,  0.5155, -0.5424,  ..., -0.7737,  0.4704, -0.4685],\n",
      "          [ 0.3587, -0.0935, -0.0988,  ..., -0.0954,  0.2611,  0.3520]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2473,  0.3493, -0.0972,  ..., -0.3204,  0.8826,  0.2929],\n",
      "          [ 0.5200, -0.6149,  0.2438,  ...,  0.0012,  0.5885, -0.3886]],\n",
      "\n",
      "         [[ 0.4681,  0.2857,  0.1056,  ..., -0.1060,  1.0992,  0.1821],\n",
      "          [ 0.9154,  0.3225,  0.5533,  ...,  0.9310,  0.1864, -0.3146]],\n",
      "\n",
      "         [[ 0.2001,  0.3239,  0.3363,  ..., -0.6182,  0.1649, -1.0363],\n",
      "          [ 0.6499,  0.2176, -0.5196,  ...,  0.3579,  0.2866,  0.1008]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.0563, -2.1479,  0.9149,  ..., -0.1527, -0.5602,  1.4918],\n",
      "         [-0.0316, -0.5712,  0.4234,  ..., -1.1844, -0.3044,  0.9887],\n",
      "         [-0.7347, -0.4508,  1.3795,  ..., -1.3949, -1.8731,  0.6145],\n",
      "         ...,\n",
      "         [ 0.7337, -0.5086, -0.3855,  ...,  0.4048, -1.1635,  1.5633],\n",
      "         [-0.4265, -1.3649, -0.9071,  ...,  0.8776, -1.3722,  0.2098],\n",
      "         [-0.4616, -0.7873,  0.4645,  ..., -0.7219, -0.9503,  0.0068]],\n",
      "\n",
      "        [[-0.0563, -2.1479,  0.9149,  ..., -0.1527, -0.5602,  1.4918],\n",
      "         [-0.2146,  0.2368,  0.9101,  ..., -0.2310,  0.7356,  0.9070],\n",
      "         [ 0.5375, -1.4821,  1.2080,  ..., -0.4145, -1.3056,  3.2342],\n",
      "         ...,\n",
      "         [ 0.4696,  0.3898,  0.7087,  ...,  1.0850,  0.3990,  0.4015],\n",
      "         [ 1.0661, -1.3273, -1.3211,  ..., -0.7212,  0.8344,  0.5444],\n",
      "         [-0.4616, -0.7873,  0.4645,  ..., -0.7219, -0.9503,  0.0068]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.0563, -2.1479,  0.9149,  ..., -0.1527, -0.5602,  1.4918],\n",
      "         [-0.0316, -0.5712,  0.4234,  ..., -1.1844, -0.3044,  0.9887],\n",
      "         [-0.7347, -0.4508,  1.3795,  ..., -1.3949, -1.8731,  0.6145],\n",
      "         ...,\n",
      "         [ 0.7337, -0.5086, -0.3855,  ...,  0.4048, -1.1635,  1.5633],\n",
      "         [-0.4265, -1.3649, -0.9071,  ...,  0.8776, -1.3722,  0.2098],\n",
      "         [-0.4616, -0.7873,  0.4645,  ..., -0.7219, -0.9503,  0.0068]],\n",
      "\n",
      "        [[-0.0563, -2.1479,  0.9149,  ..., -0.1527, -0.5602,  1.4918],\n",
      "         [-0.2146,  0.2368,  0.9101,  ..., -0.2310,  0.7356,  0.9070],\n",
      "         [ 0.5375, -1.4821,  1.2080,  ..., -0.4145, -1.3056,  3.2342],\n",
      "         ...,\n",
      "         [ 0.4696,  0.3898,  0.7087,  ...,  1.0850,  0.3990,  0.4015],\n",
      "         [ 1.0661, -1.3273, -1.3211,  ..., -0.7212,  0.8344,  0.5444],\n",
      "         [-0.4616, -0.7873,  0.4645,  ..., -0.7219, -0.9503,  0.0068]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-0.0563, -2.1479,  0.9149,  ..., -0.9934,  1.2491,  0.0331],\n",
      "          [-1.1584, -0.4644,  0.6663,  ..., -0.1527, -0.5602,  1.4918]],\n",
      "\n",
      "         [[-0.0316, -0.5712,  0.4234,  ...,  0.4291,  0.1014, -1.0291],\n",
      "          [-0.1357,  1.1548, -0.5335,  ..., -1.1844, -0.3044,  0.9887]],\n",
      "\n",
      "         [[-0.7347, -0.4508,  1.3795,  ...,  0.0782, -0.3690, -0.2144],\n",
      "          [-0.6107,  1.4333, -0.7125,  ..., -1.3949, -1.8731,  0.6145]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7337, -0.5086, -0.3855,  ..., -0.5051,  1.0750, -2.7736],\n",
      "          [-0.2291, -0.9444,  0.8794,  ...,  0.4048, -1.1635,  1.5633]],\n",
      "\n",
      "         [[-0.4265, -1.3649, -0.9071,  ...,  1.3970, -0.5203,  0.4123],\n",
      "          [-1.6151,  1.6154,  1.6746,  ...,  0.8776, -1.3722,  0.2098]],\n",
      "\n",
      "         [[-0.4616, -0.7873,  0.4645,  ..., -1.2150,  0.5388, -1.0988],\n",
      "          [-0.1985, -0.8292,  0.1394,  ..., -0.7219, -0.9503,  0.0068]]],\n",
      "\n",
      "\n",
      "        [[[-0.0563, -2.1479,  0.9149,  ..., -0.9934,  1.2491,  0.0331],\n",
      "          [-1.1584, -0.4644,  0.6663,  ..., -0.1527, -0.5602,  1.4918]],\n",
      "\n",
      "         [[-0.2146,  0.2368,  0.9101,  ..., -0.3753,  0.1119, -0.1480],\n",
      "          [-0.6359,  1.1098, -1.0854,  ..., -0.2310,  0.7356,  0.9070]],\n",
      "\n",
      "         [[ 0.5375, -1.4821,  1.2080,  ...,  0.8238,  0.5080, -0.3427],\n",
      "          [-1.1817,  2.0621, -1.2770,  ..., -0.4145, -1.3056,  3.2342]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4696,  0.3898,  0.7087,  ...,  0.2158,  1.7750, -2.2674],\n",
      "          [ 0.1300,  0.3659,  0.2227,  ...,  1.0850,  0.3990,  0.4015]],\n",
      "\n",
      "         [[ 1.0661, -1.3273, -1.3211,  ...,  0.6573, -0.5285, -0.0185],\n",
      "          [-0.6154,  0.9360,  1.2623,  ..., -0.7212,  0.8344,  0.5444]],\n",
      "\n",
      "         [[-0.4616, -0.7873,  0.4645,  ..., -1.2150,  0.5388, -1.0988],\n",
      "          [-0.1985, -0.8292,  0.1394,  ..., -0.7219, -0.9503,  0.0068]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[ 0.0882, -0.2769,  0.0466,  ...,  0.0724,  0.5560, -0.5214],\n",
      "          [-0.0862, -0.4300,  0.0955,  ...,  0.0948,  0.4464, -0.5006],\n",
      "          [ 0.0189, -0.4108,  0.1151,  ...,  0.0457,  0.4997, -0.5305],\n",
      "          ...,\n",
      "          [-0.0247, -0.3580,  0.0804,  ...,  0.0456,  0.5370, -0.5495],\n",
      "          [-0.1371, -0.3884,  0.0411,  ...,  0.1035,  0.4692, -0.4887],\n",
      "          [-0.0570, -0.3770,  0.0883,  ...,  0.0430,  0.5223, -0.5261]],\n",
      "\n",
      "         [[-0.2052,  0.9379, -0.1502,  ..., -0.5850, -0.9552,  0.6718],\n",
      "          [-0.2392,  0.9172, -0.1257,  ..., -0.5802, -0.9913,  0.6778],\n",
      "          [-0.2195,  0.8712, -0.1134,  ..., -0.5857, -0.9434,  0.6824],\n",
      "          ...,\n",
      "          [-0.2262,  0.8783, -0.2256,  ..., -0.5313, -0.9119,  0.6856],\n",
      "          [-0.2377,  0.8797, -0.1220,  ..., -0.5459, -0.9719,  0.6765],\n",
      "          [-0.2561,  0.8837, -0.0998,  ..., -0.5584, -0.9703,  0.6850]]],\n",
      "\n",
      "\n",
      "        [[[-0.9593, -0.7387,  0.8748,  ...,  0.3820,  1.3776, -0.4165],\n",
      "          [-0.4229, -0.6636,  0.6858,  ...,  0.1300,  1.1172, -0.5158],\n",
      "          [-0.3443, -0.6019,  0.6386,  ...,  0.0879,  1.0667, -0.5588],\n",
      "          ...,\n",
      "          [-0.3514, -0.5903,  0.6606,  ...,  0.0926,  1.0873, -0.5631],\n",
      "          [-0.3335, -0.5880,  0.6188,  ...,  0.0930,  1.0588, -0.5369],\n",
      "          [-0.3242, -0.6286,  0.6231,  ...,  0.0953,  1.0609, -0.5411]],\n",
      "\n",
      "         [[-0.4214,  1.0479, -0.8208,  ..., -0.1914, -0.3267,  0.5470],\n",
      "          [-0.3939,  0.7235, -0.3984,  ..., -0.2366, -0.4256,  0.5531],\n",
      "          [-0.4578,  0.7008, -0.3437,  ..., -0.3458, -0.5883,  0.5316],\n",
      "          ...,\n",
      "          [-0.4494,  0.8156, -0.4891,  ..., -0.2868, -0.4663,  0.5460],\n",
      "          [-0.3996,  0.7444, -0.4512,  ..., -0.1953, -0.4447,  0.5945],\n",
      "          [-0.4004,  0.7009, -0.3696,  ..., -0.2421, -0.4840,  0.5516]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[ 0.0882, -0.2769,  0.0466,  ...,  0.0724,  0.5560, -0.5214],\n",
      "          [-0.2052,  0.9379, -0.1502,  ..., -0.5850, -0.9552,  0.6718]],\n",
      "\n",
      "         [[-0.0862, -0.4300,  0.0955,  ...,  0.0948,  0.4464, -0.5006],\n",
      "          [-0.2392,  0.9172, -0.1257,  ..., -0.5802, -0.9913,  0.6778]],\n",
      "\n",
      "         [[ 0.0189, -0.4108,  0.1151,  ...,  0.0457,  0.4997, -0.5305],\n",
      "          [-0.2195,  0.8712, -0.1134,  ..., -0.5857, -0.9434,  0.6824]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0247, -0.3580,  0.0804,  ...,  0.0456,  0.5370, -0.5495],\n",
      "          [-0.2262,  0.8783, -0.2256,  ..., -0.5313, -0.9119,  0.6856]],\n",
      "\n",
      "         [[-0.1371, -0.3884,  0.0411,  ...,  0.1035,  0.4692, -0.4887],\n",
      "          [-0.2377,  0.8797, -0.1220,  ..., -0.5459, -0.9719,  0.6765]],\n",
      "\n",
      "         [[-0.0570, -0.3770,  0.0883,  ...,  0.0430,  0.5223, -0.5261],\n",
      "          [-0.2561,  0.8837, -0.0998,  ..., -0.5584, -0.9703,  0.6850]]],\n",
      "\n",
      "\n",
      "        [[[-0.9593, -0.7387,  0.8748,  ...,  0.3820,  1.3776, -0.4165],\n",
      "          [-0.4214,  1.0479, -0.8208,  ..., -0.1914, -0.3267,  0.5470]],\n",
      "\n",
      "         [[-0.4229, -0.6636,  0.6858,  ...,  0.1300,  1.1172, -0.5158],\n",
      "          [-0.3939,  0.7235, -0.3984,  ..., -0.2366, -0.4256,  0.5531]],\n",
      "\n",
      "         [[-0.3443, -0.6019,  0.6386,  ...,  0.0879,  1.0667, -0.5588],\n",
      "          [-0.4578,  0.7008, -0.3437,  ..., -0.3458, -0.5883,  0.5316]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3514, -0.5903,  0.6606,  ...,  0.0926,  1.0873, -0.5631],\n",
      "          [-0.4494,  0.8156, -0.4891,  ..., -0.2868, -0.4663,  0.5460]],\n",
      "\n",
      "         [[-0.3335, -0.5880,  0.6188,  ...,  0.0930,  1.0588, -0.5369],\n",
      "          [-0.3996,  0.7444, -0.4512,  ..., -0.1953, -0.4447,  0.5945]],\n",
      "\n",
      "         [[-0.3242, -0.6286,  0.6231,  ...,  0.0953,  1.0609, -0.5411],\n",
      "          [-0.4004,  0.7009, -0.3696,  ..., -0.2421, -0.4840,  0.5516]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_query\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_self_key\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_0_attention_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_self_query\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3296, -1.4549,  0.0326,  ..., -0.1574, -0.4224,  1.6893],\n",
      "         [-0.2606, -0.4572, -0.4059,  ..., -0.9441, -0.2051,  1.1130],\n",
      "         [-0.8559, -0.3861,  0.5973,  ..., -1.0750, -1.3108,  0.5718],\n",
      "         ...,\n",
      "         [ 0.5808, -0.4137, -0.9580,  ...,  0.4668, -0.8509,  1.8223],\n",
      "         [-0.5850, -0.9949, -1.2728,  ...,  1.0410, -0.9790,  0.1326],\n",
      "         [-0.6275, -0.6316, -0.3701,  ..., -0.6253, -0.7213, -0.1099]],\n",
      "\n",
      "        [[-0.7600, -1.5700, -0.0439,  ..., -0.0399, -0.3801,  1.6567],\n",
      "         [-0.8535,  0.1439, -0.0452,  ..., -0.1229,  0.9606,  0.9693],\n",
      "         [-0.2698, -1.1185,  0.2769,  ..., -0.2608, -0.9815,  3.9285],\n",
      "         ...,\n",
      "         [-0.3493,  0.3385, -0.2614,  ...,  1.5694,  0.4818,  0.2894],\n",
      "         [ 0.2867, -1.0863, -1.5508,  ..., -0.5524,  1.0603,  0.4579],\n",
      "         [-0.9856, -0.7443, -0.4620,  ..., -0.5505, -0.7681, -0.1295]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.8972, -1.6752, -0.4761,  ..., -1.1622,  2.5607,  1.5923],\n",
      "         [-1.0040, -0.5314, -0.2051,  ..., -0.1429,  1.0342,  0.7892],\n",
      "         [ 0.2916, -0.6416,  0.1751,  ..., -0.9617, -0.0383,  1.1805],\n",
      "         ...,\n",
      "         [-0.6159, -0.4890, -0.0095,  ...,  0.1764,  0.5230,  1.0938],\n",
      "         [-0.3601, -0.7853,  0.0167,  ..., -0.2099,  0.9896,  1.5234],\n",
      "         [-0.0462, -0.9612, -0.0985,  ..., -0.3869,  0.5776,  0.8768]],\n",
      "\n",
      "        [[-0.6744, -1.6437, -0.6988,  ..., -1.2316,  2.6883,  1.6881],\n",
      "         [-0.2797, -0.2150, -0.4022,  ..., -0.1376,  1.1265,  0.9428],\n",
      "         [ 0.0759, -0.2251,  0.2313,  ..., -0.7127,  0.5692,  1.3868],\n",
      "         ...,\n",
      "         [ 0.2972, -0.1666, -0.2477,  ..., -0.3310,  0.4939,  0.2635],\n",
      "         [-0.1519, -0.6049, -0.2206,  ...,  0.0731,  0.7758,  1.3523],\n",
      "         [ 0.0702, -0.9678, -0.2620,  ..., -0.4979,  0.6368,  0.8800]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.8972, -1.6752, -0.4761,  ..., -1.1622,  2.5607,  1.5923],\n",
      "         [-1.0040, -0.5314, -0.2051,  ..., -0.1429,  1.0342,  0.7892],\n",
      "         [ 0.2916, -0.6416,  0.1751,  ..., -0.9617, -0.0383,  1.1805],\n",
      "         ...,\n",
      "         [-0.6159, -0.4890, -0.0095,  ...,  0.1764,  0.5230,  1.0938],\n",
      "         [-0.3601, -0.7853,  0.0167,  ..., -0.2099,  0.9896,  1.5234],\n",
      "         [-0.0462, -0.9612, -0.0985,  ..., -0.3869,  0.5776,  0.8768]],\n",
      "\n",
      "        [[-0.6744, -1.6437, -0.6988,  ..., -1.2316,  2.6883,  1.6881],\n",
      "         [-0.2797, -0.2150, -0.4022,  ..., -0.1376,  1.1265,  0.9428],\n",
      "         [ 0.0759, -0.2251,  0.2313,  ..., -0.7127,  0.5692,  1.3868],\n",
      "         ...,\n",
      "         [ 0.2972, -0.1666, -0.2477,  ..., -0.3310,  0.4939,  0.2635],\n",
      "         [-0.1519, -0.6049, -0.2206,  ...,  0.0731,  0.7758,  1.3523],\n",
      "         [ 0.0702, -0.9678, -0.2620,  ..., -0.4979,  0.6368,  0.8800]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.8972, -1.6752, -0.4761,  ...,  0.5600,  1.9215,  1.7354],\n",
      "          [ 0.4401,  0.6091, -0.4397,  ..., -1.1622,  2.5607,  1.5923]],\n",
      "\n",
      "         [[-1.0040, -0.5314, -0.2051,  ...,  0.2528,  0.9021,  1.0833],\n",
      "          [-0.0763,  0.3166, -0.7859,  ..., -0.1429,  1.0342,  0.7892]],\n",
      "\n",
      "         [[ 0.2916, -0.6416,  0.1751,  ..., -0.3134,  1.1691, -0.2241],\n",
      "          [ 0.2964, -0.2174, -0.8999,  ..., -0.9617, -0.0383,  1.1805]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6159, -0.4890, -0.0095,  ...,  0.0644,  1.1451,  0.5024],\n",
      "          [ 1.0951,  0.7201, -0.8677,  ...,  0.1764,  0.5230,  1.0938]],\n",
      "\n",
      "         [[-0.3601, -0.7853,  0.0167,  ...,  1.0998,  1.2833,  0.4398],\n",
      "          [-0.1125,  0.2376, -0.8694,  ..., -0.2099,  0.9896,  1.5234]],\n",
      "\n",
      "         [[-0.0462, -0.9612, -0.0985,  ...,  0.0032,  0.7733,  0.4966],\n",
      "          [ 0.8479,  0.4576, -0.7463,  ..., -0.3869,  0.5776,  0.8768]]],\n",
      "\n",
      "\n",
      "        [[[-0.6744, -1.6437, -0.6988,  ...,  0.2387,  1.7539,  1.7015],\n",
      "          [ 0.3293,  0.5157, -0.1892,  ..., -1.2316,  2.6883,  1.6881]],\n",
      "\n",
      "         [[-0.2797, -0.2150, -0.4022,  ..., -0.1610,  0.8566,  1.4732],\n",
      "          [ 0.1335,  0.7631, -0.4906,  ..., -0.1376,  1.1265,  0.9428]],\n",
      "\n",
      "         [[ 0.0759, -0.2251,  0.2313,  ...,  0.3370,  0.8677, -0.1452],\n",
      "          [ 0.3702,  0.6188, -0.7835,  ..., -0.7127,  0.5692,  1.3868]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2972, -0.1666, -0.2477,  ...,  0.5551,  0.6680,  0.1935],\n",
      "          [ 1.1554,  0.0754, -1.0989,  ..., -0.3310,  0.4939,  0.2635]],\n",
      "\n",
      "         [[-0.1519, -0.6049, -0.2206,  ...,  0.4495,  1.2901,  0.6948],\n",
      "          [-0.1043,  0.4199, -0.3616,  ...,  0.0731,  0.7758,  1.3523]],\n",
      "\n",
      "         [[ 0.0702, -0.9678, -0.2620,  ..., -0.1743,  0.6734,  0.4955],\n",
      "          [ 0.7843,  0.3963, -0.6546,  ..., -0.4979,  0.6368,  0.8800]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3296, -1.4549,  0.0326,  ..., -0.1574, -0.4224,  1.6893],\n",
      "         [-0.2606, -0.4572, -0.4059,  ..., -0.9441, -0.2051,  1.1130],\n",
      "         [-0.8559, -0.3861,  0.5973,  ..., -1.0750, -1.3108,  0.5718],\n",
      "         ...,\n",
      "         [ 0.5808, -0.4137, -0.9580,  ...,  0.4668, -0.8509,  1.8223],\n",
      "         [-0.5850, -0.9949, -1.2728,  ...,  1.0410, -0.9790,  0.1326],\n",
      "         [-0.6275, -0.6316, -0.3701,  ..., -0.6253, -0.7213, -0.1099]],\n",
      "\n",
      "        [[-0.7600, -1.5700, -0.0439,  ..., -0.0399, -0.3801,  1.6567],\n",
      "         [-0.8535,  0.1439, -0.0452,  ..., -0.1229,  0.9606,  0.9693],\n",
      "         [-0.2698, -1.1185,  0.2769,  ..., -0.2608, -0.9815,  3.9285],\n",
      "         ...,\n",
      "         [-0.3493,  0.3385, -0.2614,  ...,  1.5694,  0.4818,  0.2894],\n",
      "         [ 0.2867, -1.0863, -1.5508,  ..., -0.5524,  1.0603,  0.4579],\n",
      "         [-0.9856, -0.7443, -0.4620,  ..., -0.5505, -0.7681, -0.1295]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3296, -1.4549,  0.0326,  ..., -0.1574, -0.4224,  1.6893],\n",
      "         [-0.2606, -0.4572, -0.4059,  ..., -0.9441, -0.2051,  1.1130],\n",
      "         [-0.8559, -0.3861,  0.5973,  ..., -1.0750, -1.3108,  0.5718],\n",
      "         ...,\n",
      "         [ 0.5808, -0.4137, -0.9580,  ...,  0.4668, -0.8509,  1.8223],\n",
      "         [-0.5850, -0.9949, -1.2728,  ...,  1.0410, -0.9790,  0.1326],\n",
      "         [-0.6275, -0.6316, -0.3701,  ..., -0.6253, -0.7213, -0.1099]],\n",
      "\n",
      "        [[-0.7600, -1.5700, -0.0439,  ..., -0.0399, -0.3801,  1.6567],\n",
      "         [-0.8535,  0.1439, -0.0452,  ..., -0.1229,  0.9606,  0.9693],\n",
      "         [-0.2698, -1.1185,  0.2769,  ..., -0.2608, -0.9815,  3.9285],\n",
      "         ...,\n",
      "         [-0.3493,  0.3385, -0.2614,  ...,  1.5694,  0.4818,  0.2894],\n",
      "         [ 0.2867, -1.0863, -1.5508,  ..., -0.5524,  1.0603,  0.4579],\n",
      "         [-0.9856, -0.7443, -0.4620,  ..., -0.5505, -0.7681, -0.1295]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-0.3296, -1.4549,  0.0326,  ..., -0.9630,  1.0327,  0.0334],\n",
      "          [-0.6373, -0.1267,  1.1279,  ..., -0.1574, -0.4224,  1.6893]],\n",
      "\n",
      "         [[-0.2606, -0.4572, -0.4059,  ...,  0.0864, -0.2685, -0.8241],\n",
      "          [ 0.2039,  1.8992, -0.2168,  ..., -0.9441, -0.2051,  1.1130]],\n",
      "\n",
      "         [[-0.8559, -0.3861,  0.5973,  ..., -0.2662, -0.6495, -0.1928],\n",
      "          [-0.2498,  2.3416, -0.4206,  ..., -1.0750, -1.3108,  0.5718]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5808, -0.4137, -0.9580,  ..., -0.7200,  0.8273, -1.8505],\n",
      "          [ 0.1198, -0.5769,  1.4634,  ...,  0.4668, -0.8509,  1.8223]],\n",
      "\n",
      "         [[-0.5850, -0.9949, -1.2728,  ...,  1.3656, -0.7662,  0.4799],\n",
      "          [-0.9832,  2.5045,  2.5843,  ...,  1.0410, -0.9790,  0.1326]],\n",
      "\n",
      "         [[-0.6275, -0.6316, -0.3701,  ..., -1.1500,  0.2010, -0.8732],\n",
      "          [ 0.1426, -0.4746,  0.5108,  ..., -0.6253, -0.7213, -0.1099]]],\n",
      "\n",
      "\n",
      "        [[[-0.7600, -1.5700, -0.0439,  ..., -0.8371,  0.8585, -0.3569],\n",
      "          [-0.5992, -0.6395,  0.5322,  ..., -0.0399, -0.3801,  1.6567]],\n",
      "\n",
      "         [[-0.8535,  0.1439, -0.0452,  ..., -0.5473, -0.2735, -0.3501],\n",
      "          [-0.2343,  1.2509, -0.8969,  ..., -0.1229,  0.9606,  0.9693]],\n",
      "\n",
      "         [[-0.2698, -1.1185,  0.2769,  ...,  0.6924,  0.1194, -0.4618],\n",
      "          [-0.6586,  2.5283, -0.9539,  ..., -0.2608, -0.9815,  3.9285]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3493,  0.3385, -0.2614,  ...,  0.0151,  1.7791, -1.7200],\n",
      "          [ 0.6387,  0.2899,  0.2363,  ...,  1.5694,  0.4818,  0.2894]],\n",
      "\n",
      "         [[ 0.2867, -1.0863, -1.5508,  ...,  0.4898, -0.7796, -0.2158],\n",
      "          [-0.2220,  0.9668,  1.5709,  ..., -0.5524,  1.0603,  0.4579]],\n",
      "\n",
      "         [[-0.9856, -0.7443, -0.4620,  ..., -1.0797,  0.1531, -0.9869],\n",
      "          [ 0.2069, -0.7643,  0.1525,  ..., -0.5505, -0.7681, -0.1295]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.3296, -1.4549,  0.0326,  ..., -0.1574, -0.4224,  1.6893],\n",
      "         [-0.2606, -0.4572, -0.4059,  ..., -0.9441, -0.2051,  1.1130],\n",
      "         [-0.8559, -0.3861,  0.5973,  ..., -1.0750, -1.3108,  0.5718],\n",
      "         ...,\n",
      "         [ 0.5808, -0.4137, -0.9580,  ...,  0.4668, -0.8509,  1.8223],\n",
      "         [-0.5850, -0.9949, -1.2728,  ...,  1.0410, -0.9790,  0.1326],\n",
      "         [-0.6275, -0.6316, -0.3701,  ..., -0.6253, -0.7213, -0.1099]],\n",
      "\n",
      "        [[-0.7600, -1.5700, -0.0439,  ..., -0.0399, -0.3801,  1.6567],\n",
      "         [-0.8535,  0.1439, -0.0452,  ..., -0.1229,  0.9606,  0.9693],\n",
      "         [-0.2698, -1.1185,  0.2769,  ..., -0.2608, -0.9815,  3.9285],\n",
      "         ...,\n",
      "         [-0.3493,  0.3385, -0.2614,  ...,  1.5694,  0.4818,  0.2894],\n",
      "         [ 0.2867, -1.0863, -1.5508,  ..., -0.5524,  1.0603,  0.4579],\n",
      "         [-0.9856, -0.7443, -0.4620,  ..., -0.5505, -0.7681, -0.1295]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[-0.3296, -1.4549,  0.0326,  ..., -0.1574, -0.4224,  1.6893],\n",
      "         [-0.2606, -0.4572, -0.4059,  ..., -0.9441, -0.2051,  1.1130],\n",
      "         [-0.8559, -0.3861,  0.5973,  ..., -1.0750, -1.3108,  0.5718],\n",
      "         ...,\n",
      "         [ 0.5808, -0.4137, -0.9580,  ...,  0.4668, -0.8509,  1.8223],\n",
      "         [-0.5850, -0.9949, -1.2728,  ...,  1.0410, -0.9790,  0.1326],\n",
      "         [-0.6275, -0.6316, -0.3701,  ..., -0.6253, -0.7213, -0.1099]],\n",
      "\n",
      "        [[-0.7600, -1.5700, -0.0439,  ..., -0.0399, -0.3801,  1.6567],\n",
      "         [-0.8535,  0.1439, -0.0452,  ..., -0.1229,  0.9606,  0.9693],\n",
      "         [-0.2698, -1.1185,  0.2769,  ..., -0.2608, -0.9815,  3.9285],\n",
      "         ...,\n",
      "         [-0.3493,  0.3385, -0.2614,  ...,  1.5694,  0.4818,  0.2894],\n",
      "         [ 0.2867, -1.0863, -1.5508,  ..., -0.5524,  1.0603,  0.4579],\n",
      "         [-0.9856, -0.7443, -0.4620,  ..., -0.5505, -0.7681, -0.1295]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[[-0.3296, -1.4549,  0.0326,  ..., -0.9630,  1.0327,  0.0334],\n",
      "          [-0.6373, -0.1267,  1.1279,  ..., -0.1574, -0.4224,  1.6893]],\n",
      "\n",
      "         [[-0.2606, -0.4572, -0.4059,  ...,  0.0864, -0.2685, -0.8241],\n",
      "          [ 0.2039,  1.8992, -0.2168,  ..., -0.9441, -0.2051,  1.1130]],\n",
      "\n",
      "         [[-0.8559, -0.3861,  0.5973,  ..., -0.2662, -0.6495, -0.1928],\n",
      "          [-0.2498,  2.3416, -0.4206,  ..., -1.0750, -1.3108,  0.5718]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5808, -0.4137, -0.9580,  ..., -0.7200,  0.8273, -1.8505],\n",
      "          [ 0.1198, -0.5769,  1.4634,  ...,  0.4668, -0.8509,  1.8223]],\n",
      "\n",
      "         [[-0.5850, -0.9949, -1.2728,  ...,  1.3656, -0.7662,  0.4799],\n",
      "          [-0.9832,  2.5045,  2.5843,  ...,  1.0410, -0.9790,  0.1326]],\n",
      "\n",
      "         [[-0.6275, -0.6316, -0.3701,  ..., -1.1500,  0.2010, -0.8732],\n",
      "          [ 0.1426, -0.4746,  0.5108,  ..., -0.6253, -0.7213, -0.1099]]],\n",
      "\n",
      "\n",
      "        [[[-0.7600, -1.5700, -0.0439,  ..., -0.8371,  0.8585, -0.3569],\n",
      "          [-0.5992, -0.6395,  0.5322,  ..., -0.0399, -0.3801,  1.6567]],\n",
      "\n",
      "         [[-0.8535,  0.1439, -0.0452,  ..., -0.5473, -0.2735, -0.3501],\n",
      "          [-0.2343,  1.2509, -0.8969,  ..., -0.1229,  0.9606,  0.9693]],\n",
      "\n",
      "         [[-0.2698, -1.1185,  0.2769,  ...,  0.6924,  0.1194, -0.4618],\n",
      "          [-0.6586,  2.5283, -0.9539,  ..., -0.2608, -0.9815,  3.9285]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3493,  0.3385, -0.2614,  ...,  0.0151,  1.7791, -1.7200],\n",
      "          [ 0.6387,  0.2899,  0.2363,  ...,  1.5694,  0.4818,  0.2894]],\n",
      "\n",
      "         [[ 0.2867, -1.0863, -1.5508,  ...,  0.4898, -0.7796, -0.2158],\n",
      "          [-0.2220,  0.9668,  1.5709,  ..., -0.5524,  1.0603,  0.4579]],\n",
      "\n",
      "         [[-0.9856, -0.7443, -0.4620,  ..., -1.0797,  0.1531, -0.9869],\n",
      "          [ 0.2069, -0.7643,  0.1525,  ..., -0.5505, -0.7681, -0.1295]]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[[-0.0813, -0.4701, -0.7654,  ...,  0.2147, -0.0440, -0.2155],\n",
      "          [-0.3841, -0.2296, -0.6111,  ..., -0.1429,  0.1293, -0.4470],\n",
      "          [-0.2150, -0.3513, -0.6560,  ..., -0.0275,  0.0505, -0.4078],\n",
      "          ...,\n",
      "          [-0.3079, -0.4045, -0.5999,  ...,  0.1177, -0.1382, -0.3617],\n",
      "          [-0.2207, -0.3519, -0.6236,  ..., -0.0111,  0.0461, -0.3809],\n",
      "          [-0.2948, -0.4136, -0.6534,  ...,  0.0108,  0.0763, -0.3897]],\n",
      "\n",
      "         [[ 0.0945,  1.5102,  0.0562,  ..., -0.4731, -0.5565,  0.8772],\n",
      "          [ 0.0266,  1.1343,  0.4129,  ..., -0.3348, -0.6370,  0.8746],\n",
      "          [ 0.0092,  1.2561,  0.5020,  ..., -0.3190, -0.6921,  0.8437],\n",
      "          ...,\n",
      "          [ 0.1097,  1.3359,  0.3187,  ..., -0.3475, -0.5890,  0.7973],\n",
      "          [ 0.0135,  1.3205,  0.5053,  ..., -0.3335, -0.6649,  0.7986],\n",
      "          [ 0.0594,  1.2804,  0.3521,  ..., -0.4132, -0.6551,  0.8721]]],\n",
      "\n",
      "\n",
      "        [[[-0.4021, -0.5820, -0.5805,  ..., -0.0445,  0.4024, -0.5221],\n",
      "          [-0.4037, -0.5128, -0.5200,  ..., -0.1959,  0.3657, -0.6731],\n",
      "          [-0.5614, -0.3133, -0.4040,  ..., -0.1769,  0.3861, -0.7782],\n",
      "          ...,\n",
      "          [-0.3457, -0.6424, -0.6871,  ...,  0.0325,  0.1323, -0.5579],\n",
      "          [-0.5064, -0.3794, -0.4174,  ..., -0.1433,  0.4411, -0.7102],\n",
      "          [-0.4457, -0.7195, -0.5950,  ..., -0.0645,  0.3061, -0.5607]],\n",
      "\n",
      "         [[-0.0557,  1.0037, -0.1810,  ..., -0.0801, -0.2580,  0.5720],\n",
      "          [ 0.0937,  0.5675,  0.1876,  ..., -0.0993, -0.3536,  0.6553],\n",
      "          [-0.1531,  0.9749, -0.1459,  ..., -0.1461, -0.3360,  1.0193],\n",
      "          ...,\n",
      "          [ 0.0063,  0.8991, -0.2152,  ..., -0.0522, -0.2946,  0.9091],\n",
      "          [-0.1519,  0.8741,  0.0551,  ..., -0.2453, -0.3539,  0.8716],\n",
      "          [-0.0944,  0.9006, -0.1755,  ..., -0.0776, -0.2600,  1.0065]]]],\n",
      "       grad_fn=<ScaledDotProductFlashAttentionForCpuBackward0>)\n",
      "tensor([[[[-0.0813, -0.4701, -0.7654,  ...,  0.2147, -0.0440, -0.2155],\n",
      "          [ 0.0945,  1.5102,  0.0562,  ..., -0.4731, -0.5565,  0.8772]],\n",
      "\n",
      "         [[-0.3841, -0.2296, -0.6111,  ..., -0.1429,  0.1293, -0.4470],\n",
      "          [ 0.0266,  1.1343,  0.4129,  ..., -0.3348, -0.6370,  0.8746]],\n",
      "\n",
      "         [[-0.2150, -0.3513, -0.6560,  ..., -0.0275,  0.0505, -0.4078],\n",
      "          [ 0.0092,  1.2561,  0.5020,  ..., -0.3190, -0.6921,  0.8437]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3079, -0.4045, -0.5999,  ...,  0.1177, -0.1382, -0.3617],\n",
      "          [ 0.1097,  1.3359,  0.3187,  ..., -0.3475, -0.5890,  0.7973]],\n",
      "\n",
      "         [[-0.2207, -0.3519, -0.6236,  ..., -0.0111,  0.0461, -0.3809],\n",
      "          [ 0.0135,  1.3205,  0.5053,  ..., -0.3335, -0.6649,  0.7986]],\n",
      "\n",
      "         [[-0.2948, -0.4136, -0.6534,  ...,  0.0108,  0.0763, -0.3897],\n",
      "          [ 0.0594,  1.2804,  0.3521,  ..., -0.4132, -0.6551,  0.8721]]],\n",
      "\n",
      "\n",
      "        [[[-0.4021, -0.5820, -0.5805,  ..., -0.0445,  0.4024, -0.5221],\n",
      "          [-0.0557,  1.0037, -0.1810,  ..., -0.0801, -0.2580,  0.5720]],\n",
      "\n",
      "         [[-0.4037, -0.5128, -0.5200,  ..., -0.1959,  0.3657, -0.6731],\n",
      "          [ 0.0937,  0.5675,  0.1876,  ..., -0.0993, -0.3536,  0.6553]],\n",
      "\n",
      "         [[-0.5614, -0.3133, -0.4040,  ..., -0.1769,  0.3861, -0.7782],\n",
      "          [-0.1531,  0.9749, -0.1459,  ..., -0.1461, -0.3360,  1.0193]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3457, -0.6424, -0.6871,  ...,  0.0325,  0.1323, -0.5579],\n",
      "          [ 0.0063,  0.8991, -0.2152,  ..., -0.0522, -0.2946,  0.9091]],\n",
      "\n",
      "         [[-0.5064, -0.3794, -0.4174,  ..., -0.1433,  0.4411, -0.7102],\n",
      "          [-0.1519,  0.8741,  0.0551,  ..., -0.2453, -0.3539,  0.8716]],\n",
      "\n",
      "         [[-0.4457, -0.7195, -0.5950,  ..., -0.0645,  0.3061, -0.5607],\n",
      "          [-0.0944,  0.9006, -0.1755,  ..., -0.0776, -0.2600,  1.0065]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_attention_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_encoder_layer_1_output_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: bert_pooler_dense\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mPruning module: classifier\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from chop.pipelines import CompressionPipeline\n",
    "from chop import MaseGraph\n",
    "\n",
    "mg = MaseGraph(model)\n",
    "pipe = CompressionPipeline()\n",
    "\n",
    "quantization_config = {\n",
    "    \"by\": \"type\",\n",
    "    \"default\": {\n",
    "        \"config\": {\n",
    "            \"name\": None,\n",
    "        }\n",
    "    },\n",
    "    \"linear\": {\n",
    "        \"config\": {\n",
    "            \"name\": \"integer\",\n",
    "            # data\n",
    "            \"data_in_width\": 8,\n",
    "            \"data_in_frac_width\": 4,\n",
    "            # weight\n",
    "            \"weight_width\": 8,\n",
    "            \"weight_frac_width\": 4,\n",
    "            # bias\n",
    "            \"bias_width\": 8,\n",
    "            \"bias_frac_width\": 4,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "pruning_config = {\n",
    "    \"weight\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "    \"activation\": {\n",
    "        \"sparsity\": 0.5,\n",
    "        \"method\": \"l1-norm\",\n",
    "        \"scope\": \"local\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mg, _ = pipe(\n",
    "    mg,\n",
    "    pass_args={\n",
    "        \"quantize_transform_pass\": quantization_config,\n",
    "        \"prune_transform_pass\": pruning_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, export the MaseGraph for the compressed checkpoint to be used in future tutorials for hardware generation and distributed deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.export(f\"{Path.home()}/adlsystems/tutorial_5_nas_compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to sampler_cumulative_results.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame\n",
    "df = pd.read_csv(\"sampler_cumulative_results.csv\")\n",
    "\n",
    "df_cumulative = df.cummax()\n",
    "df_cumulative[\"Trial\"] = df.index + 1\n",
    "csv_filename = \"sampler_cumulative_results.csv\"\n",
    "df_cumulative.to_csv(csv_filename, index=False)\n",
    "print(f\"Data saved to {csv_filename}\")\n",
    "df_melted = df_cumulative.melt(id_vars=[\"Trial\"], var_name=\"Sampler\", value_name=\"Best Accuracy So Far\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_melted, x=\"Trial\", y=\"Best Accuracy So Far\", hue=\"Sampler\", linewidth=2, drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"Trial Number\", fontsize=12)\n",
    "plt.ylabel(\"Best Accuracy\", fontsize=12)\n",
    "plt.title(\"Progression of Best Accuracy Over Trials\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(title=\"Sampler\")\n",
    "plt.savefig(\"Sampler_Comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32mINFO    \u001b[0m \u001b[34mTokenizing dataset imdb with AutoTokenizer for bert-base-uncased.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from chop.nn.modules import Identity\n",
    "from pathlib import Path\n",
    "import dill\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from chop.tools.utils import deepsetattr\n",
    "from chop.tools import get_tokenized_dataset\n",
    "from chop.tools import get_trainer\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from optuna.samplers import GridSampler, RandomSampler, TPESampler\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = \"prajjwal1/bert-tiny\"\n",
    "tokenizer_checkpoint = \"bert-base-uncased\"\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "\n",
    "dataset, tokenizer = get_tokenized_dataset(\n",
    "    dataset=dataset_name,\n",
    "    checkpoint=tokenizer_checkpoint,\n",
    "    return_tokenizer=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 12:23:02,784] A new study created in memory with name: bert-tiny-nas-study\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  79/3125 00:51 < 33:59, 1.49 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-02-06 12:23:56,285] Trial 0 failed with parameters: {'num_layers': 2, 'num_heads': 4, 'hidden_size': 128, 'intermediate_size': 2048, 'bert.encoder.layer.0.attention.self.query_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.0.attention.self.key_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.0.attention.self.value_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.0.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.query_type': <class 'chop.nn.modules.identity.Identity'>, 'bert.encoder.layer.1.attention.self.key_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.self.value_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.encoder.layer.1.attention.output.dense_type': <class 'torch.nn.modules.linear.Linear'>, 'bert.pooler.dense_type': <class 'chop.nn.modules.identity.Identity'>} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/tmp/pbs.178537.pbs-7/ipykernel_145045/3057174558.py\", line 80, in <lambda>\n",
      "    lambda trial: objective(trial, is_grid=True),\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/tmp/pbs.178537.pbs-7/ipykernel_145045/3057174558.py\", line 53, in objective\n",
      "    trainer.train()\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/trainer.py\", line 2171, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/trainer.py\", line 2531, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/trainer.py\", line 3675, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/trainer.py\", line 3731, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 1665, in forward\n",
      "    outputs = self.bert(\n",
      "              ^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 1142, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 695, in forward\n",
      "    layer_outputs = layer_module(\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 627, in forward\n",
      "    layer_output = apply_chunking_to_forward(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/pytorch_utils.py\", line 255, in apply_chunking_to_forward\n",
      "    return forward_fn(*input_tensors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 639, in feed_forward_chunk\n",
      "    intermediate_output = self.intermediate(attention_output)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py\", line 540, in forward\n",
      "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/jj21/home/.conda/envs/adls/lib/python3.11/site-packages/transformers/activations.py\", line 78, in forward\n",
      "    return self.act(input)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-02-06 12:23:56,301] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 79\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGridSampler\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m     75\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-tiny-nas-study\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler,\n\u001b[1;32m     78\u001b[0m )\n\u001b[0;32m---> 79\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(sampler_accuracy)\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[16], line 80\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGridSampler\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[1;32m     75\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     76\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-tiny-nas-study\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m     sampler\u001b[38;5;241m=\u001b[39msampler,\n\u001b[1;32m     78\u001b[0m )\n\u001b[1;32m     79\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m     81\u001b[0m     n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     82\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m     83\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(sampler_accuracy)\n",
      "Cell \u001b[0;32mIn[16], line 53\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, is_grid)\u001b[0m\n\u001b[1;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m construct_model(trial, is_grid)\n\u001b[1;32m     45\u001b[0m trainer \u001b[38;5;241m=\u001b[39m get_trainer(\n\u001b[1;32m     46\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     47\u001b[0m     tokenized_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     51\u001b[0m )\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     56\u001b[0m sampler_accuracy[sampler_type]\u001b[38;5;241m.\u001b[39mappend(eval_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3681\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1665\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1665\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1677\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1679\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/pytorch_utils.py:255\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:540\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/adls/lib/python3.11/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def construct_model(trial, is_grid):\n",
    "    config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "    # Update the paramaters in the config\n",
    "    for param in [\n",
    "        \"num_layers\",\n",
    "        \"num_heads\",\n",
    "        \"hidden_size\",\n",
    "        \"intermediate_size\",\n",
    "    ]:\n",
    "        if is_grid:\n",
    "            chosen_val = trial.suggest_categorical(param, search_space[param])\n",
    "            setattr(config, param, chosen_val) \n",
    "        else:\n",
    "            chosen_idx = trial.suggest_int(param, 0, len(search_space[param]) - 1)\n",
    "            setattr(config, param, search_space[param][chosen_idx])\n",
    "\n",
    "    trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    for name, layer in trial_model.named_modules():\n",
    "        if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "            new_layer_cls = trial.suggest_categorical(\n",
    "                f\"{name}_type\",\n",
    "                search_space[\"linear_layer_choices\"],\n",
    "            )\n",
    "\n",
    "            if new_layer_cls == nn.Linear:\n",
    "                continue\n",
    "            elif new_layer_cls == Identity:\n",
    "                new_layer = Identity()\n",
    "                deepsetattr(trial_model, name, new_layer)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown layer type: {new_layer_cls}\")\n",
    "\n",
    "    return trial_model\n",
    "\n",
    "\n",
    "sampler_accuracy = {\"RandomSampler\":[], \"TPESampler\":[] , \"GridSampler\":[]}\n",
    "\n",
    "def objective(trial, is_grid=False):\n",
    "\n",
    "    # Define the model\n",
    "    model = construct_model(trial, is_grid)\n",
    "\n",
    "    trainer = get_trainer(\n",
    "        model=model,\n",
    "        tokenized_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        evaluate_metric=\"accuracy\",\n",
    "        num_train_epochs=1,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    sampler_accuracy[sampler_type].append(eval_results[\"eval_accuracy\"]) \n",
    "\n",
    "    # Set the model as an attribute so we can fetch it later\n",
    "    trial.set_user_attr(\"model\", model)\n",
    "\n",
    "    return eval_results[\"eval_accuracy\"]\n",
    "\n",
    "\n",
    "sampler_type = \"GridSampler\"\n",
    "grid_search_space = search_space.copy()\n",
    "# Letting gridsampler know the layers beforehand\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "trial_model = AutoModelForSequenceClassification.from_config(config)\n",
    "for name, layer in trial_model.named_modules():\n",
    "    if isinstance(layer, nn.Linear) and layer.in_features == layer.out_features:\n",
    "        grid_search_space[f\"{name}_type\"] = grid_search_space['linear_layer_choices']\n",
    "sampler = GridSampler(grid_search_space)\n",
    "print(\"GridSampler\")\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"bert-tiny-nas-study\",\n",
    "    sampler=sampler,\n",
    ")\n",
    "study.optimize(\n",
    "    lambda trial: objective(trial, is_grid=True),\n",
    "    n_trials=100,\n",
    "    timeout=60 * 60 * 24,\n",
    ")\n",
    "print(sampler_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.11.11 adls",
   "language": "python",
   "name": "adls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
