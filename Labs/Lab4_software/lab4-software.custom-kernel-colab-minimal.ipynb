{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff1bS2_YZ3hB"
      },
      "source": [
        "# Custom Kernel Tutorial\n",
        "> Revision\n",
        "> - Created on 21/01/2025, Cheng Zhang: PyTorch 2.5.0, CUDA 12.3\n",
        "> - Fixed and Tested on 05/02/2025, Cheng Zhang: PyTorch 2.6.0, CUDA 12.5\n",
        "\n",
        "## Env Setup in Colab\n",
        "\n",
        "Check if Colab is connected to a NVIDIA Tesla T4 or Ada L4 GPU (L4 is faster), if not, change Colab runtime to T4 or L4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBGkK1R2NVyT",
        "outputId": "c6d2e2e2-97b6-487c-8379-703a6f327a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb 12 17:37:16 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('âŒ Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B66afpunzTQp"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VUhihX3iOYF9",
        "outputId": "48f42c5f-e46a-435e-81e3-d38557d4ee9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tox\n",
            "  Downloading tox-4.24.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Collecting rust-just\n",
            "  Downloading rust_just-1.39.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (120 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m120.5/120.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (8.3.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: cachetools>=5.5 in /usr/local/lib/python3.11/dist-packages (from tox) (5.5.1)\n",
            "Requirement already satisfied: chardet>=5.2 in /usr/local/lib/python3.11/dist-packages (from tox) (5.2.0)\n",
            "Collecting colorama>=0.4.6 (from tox)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: filelock>=3.16.1 in /usr/local/lib/python3.11/dist-packages (from tox) (3.17.0)\n",
            "Requirement already satisfied: packaging>=24.2 in /usr/local/lib/python3.11/dist-packages (from tox) (24.2)\n",
            "Requirement already satisfied: platformdirs>=4.3.6 in /usr/local/lib/python3.11/dist-packages (from tox) (4.3.6)\n",
            "Requirement already satisfied: pluggy>=1.5 in /usr/local/lib/python3.11/dist-packages (from tox) (1.5.0)\n",
            "Collecting pyproject-api>=1.8 (from tox)\n",
            "  Downloading pyproject_api-1.9.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting virtualenv>=20.27.1 (from tox)\n",
            "  Downloading virtualenv-20.29.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest) (2.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.27.1->tox)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading tox-4.24.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m171.8/171.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rust_just-1.39.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading pyproject_api-1.9.0-py3-none-any.whl (13 kB)\n",
            "Downloading virtualenv-20.29.2-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distlib, virtualenv, rust-just, pyproject-api, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, colorama, tox, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed colorama-0.4.6 distlib-0.3.9 ninja-1.11.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyproject-api-1.9.0 rust-just-1.39.0 tox-4.24.1 virtualenv-20.29.2\n",
            "Collecting build\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build) (24.2)\n",
            "Collecting pyproject_hooks (from build)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: pyproject_hooks, build\n",
            "Successfully installed build-1.2.2.post1 pyproject_hooks-1.2.0\n",
            "Cloning into 'mase-cuda'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (270/270), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 270 (delta 91), reused 266 (delta 88), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (270/270), 62.99 KiB | 9.00 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Submodule 'submodules/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path 'submodules/cutlass'\n",
            "Submodule 'submodules/nlohmann_json' (https://github.com/nlohmann/json.git) registered for path 'submodules/nlohmann_json'\n",
            "Cloning into '/content/mase-cuda/submodules/cutlass'...\n",
            "remote: Enumerating objects: 31719, done.        \n",
            "remote: Counting objects: 100% (92/92), done.        \n",
            "remote: Compressing objects: 100% (74/74), done.        \n",
            "remote: Total 31719 (delta 38), reused 18 (delta 18), pack-reused 31627 (from 2)        \n",
            "Receiving objects: 100% (31719/31719), 47.97 MiB | 17.00 MiB/s, done.\n",
            "Resolving deltas: 100% (24021/24021), done.\n",
            "Cloning into '/content/mase-cuda/submodules/nlohmann_json'...\n",
            "remote: Enumerating objects: 47834, done.        \n",
            "remote: Counting objects: 100% (154/154), done.        \n",
            "remote: Compressing objects: 100% (122/122), done.        \n",
            "remote: Total 47834 (delta 86), reused 32 (delta 32), pack-reused 47680 (from 4)        \n",
            "Receiving objects: 100% (47834/47834), 200.15 MiB | 17.34 MiB/s, done.\n",
            "Resolving deltas: 100% (28483/28483), done.\n",
            "Submodule path 'submodules/cutlass': checked out '08101d9d0ca68fbdd4ed8833a9fa66dc3948b77d'\n",
            "Submodule path 'submodules/nlohmann_json': checked out 'ac8b22180db393d56f5f2954eb353967fad254e3'\n",
            "/content/mase-cuda\n"
          ]
        }
      ],
      "source": [
        "! pip install tox ninja torch numpy scipy rust-just pytest transformers\n",
        "! pip install -U build\n",
        "\n",
        "# git_token = \"Your Git Token\"\n",
        "! git clone --recurse-submodules https://github.com/DeepWok/mase-cuda.git\n",
        "%cd mase-cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCJ6j3wRaoc2"
      },
      "source": [
        "## Build & Run C++ Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fIMUKJWLJo10",
        "outputId": "32c46dbb-cee9-469b-895e-8f1285bd7786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m# python\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/dist ]; then rm -r /content/mase-cuda/dist; fi\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/src/mase_cuda.egg-info ]; then rm -r /content/mase-cuda/src/mase_cuda.egg-info; fi\u001b[0m\n",
            "\u001b[1m# all\u001b[0m\n",
            "\u001b[1mif [ -d /content/mase-cuda/build ]; then rm -r /content/mase-cuda/build; fi\u001b[0m\n",
            "\u001b[1mecho $(which cmake)\u001b[0m\n",
            "/usr/local/bin/cmake\n",
            "\u001b[1mcmake -D BUILD_TESTING=ON -D CUDA_ARCHITECTURES=native -B build -S .\u001b[0m\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- CUDA_ARCHITECTURES: native\n",
            "-- Found Python3: /usr/local/bin/python (found version \"3.11.11\") found components: Interpreter Development Development.Module Development.Embed\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.5\") \n",
            "-- Found CUDAToolkit: /usr/local/cuda/include (found version \"12.5.82\")\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Caffe2: CUDA detected: 12.5\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 12.5\n",
            "-- Found Python: /usr/local/bin/python (found version \"3.11.11\") found components: Interpreter\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:140 (message):\n",
            "  Failed to compute shorthash for libnvrtc.so\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[33mCMake Warning (dev) at /usr/local/lib/python3.11/dist-packages/cmake/data/share/cmake-3.31/Modules/FindPackageHandleStandardArgs.cmake:441 (message):\n",
            "  The package name passed to `find_package_handle_standard_args` (nvtx3) does\n",
            "  not match the name of the calling package (Caffe2).  This can lead to\n",
            "  problems in calling code that expects `find_package` result variables\n",
            "  (e.g., `_FOUND`) to follow a certain pattern.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:174 (find_package_handle_standard_args)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Could NOT find nvtx3 (missing: nvtx3_dir) \n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/public/cuda.cmake:180 (message):\n",
            "  Cannot find NVTX3, find old NVTX instead\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Caffe2/Caffe2Config.cmake:86 (include)\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- USE_CUDNN is set to 0. Compiling without cuDNN support\n",
            "-- USE_CUSPARSELT is set to 0. Compiling without cuSPARSELt support\n",
            "-- USE_CUDSS is set to 0. Compiling without cuDSS support\n",
            "-- USE_CUFILE is set to 0. Compiling without cuFile support\n",
            "-- Autodetected CUDA architecture(s):  7.5\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75\n",
            "\u001b[33mCMake Warning at /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:22 (message):\n",
            "  static library kineto_LIBRARY-NOTFOUND not found.\n",
            "Call Stack (most recent call first):\n",
            "  /usr/local/lib/python3.11/dist-packages/torch/share/cmake/Torch/TorchConfig.cmake:120 (append_torchlib_if_found)\n",
            "  CMakeLists.txt:30 (find_package)\n",
            "\n",
            "\u001b[0m\n",
            "-- Found Torch: /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch.so\n",
            "-- Configuring done (17.3s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/mase-cuda/build\n",
            "\u001b[1mif [ -z  ]; then cmake --build /content/mase-cuda/build -j 2 ; else cmake --build /content/mase-cuda/build --target  -j 2 ; fi\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CUDA object test/cu/mxint/dequantize/CMakeFiles/test_mxint8_dequantize1d.dir/test_mxint8_dequantize1d.cu.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CUDA executable test_mxint8_dequantize1d\u001b[0m\n",
            "[100%] Built target test_mxint8_dequantize1d\n"
          ]
        }
      ],
      "source": [
        "!just build-cu-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BajynrmayrY"
      },
      "source": [
        "Run test executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIxj1uQGZQcj",
        "outputId": "1806d754-ec4f-4b37-c48a-d1134c6eb96c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d [m] [group_size] [is_random]\n",
            "m=4096, group_size=128, num_groups=32, is_random=0\n",
            "PASSED\n"
          ]
        }
      ],
      "source": [
        "! ./build/test/cu/mxint/dequantize/test_mxint8_dequantize1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdO3h_06a99Q"
      },
      "source": [
        "## Build & Try mase-cuda Package\n",
        "\n",
        "The building process can be slow. NVIDIA T4's compuate capability is 7.5, and L4 is 8.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v01rAwvkZaf6",
        "outputId": "c7597c54-f235-4303-fb3a-f6d53bb7e5ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m* Creating isolated environment: venv+pip...\u001b[0m\n",
            "\u001b[1m* Installing packages in isolated environment:\u001b[0m\n",
            "  - numpy\n",
            "  - setuptools\n",
            "  - torch\n",
            "\u001b[1m* Getting build dependencies for wheel...\u001b[0m\n",
            "running egg_info\n",
            "creating src/mase_cuda.egg-info\n",
            "writing src/mase_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to src/mase_cuda.egg-info/dependency_links.txt\n",
            "writing requirements to src/mase_cuda.egg-info/requires.txt\n",
            "writing top-level names to src/mase_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "reading manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "\u001b[1m* Building wheel...\u001b[0m\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/utils.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/__init__.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "copying src/mase_cuda/constants.py -> build/lib.linux-x86_64-cpython-311/mase_cuda\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/dequantize.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/__init__.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/linear.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "copying src/mase_cuda/mxint8/quantize.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8\n",
            "creating build/lib.linux-x86_64-cpython-311/mase_cuda/tools\n",
            "copying src/mase_cuda/tools/bin_repr.py -> build/lib.linux-x86_64-cpython-311/mase_cuda/tools\n",
            "running egg_info\n",
            "writing src/mase_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to src/mase_cuda.egg-info/dependency_links.txt\n",
            "writing requirements to src/mase_cuda.egg-info/requires.txt\n",
            "writing top-level names to src/mase_cuda.egg-info/top_level.txt\n",
            "reading manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'src/mase_cuda.egg-info/SOURCES.txt'\n",
            "creating build/lib.linux-x86_64-cpython-311/csrc\n",
            "copying src/csrc/bind.cu -> build/lib.linux-x86_64-cpython-311/csrc\n",
            "running build_ext\n",
            "/tmp/build-env-dw5296y5/lib/python3.11/site-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/tmp/build-env-dw5296y5/lib/python3.11/site-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'mase_cuda_ext' extension\n",
            "creating /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc\n",
            "Emitting ninja build file /content/mase-cuda/build/temp.linux-x86_64-cpython-311/build.ninja...\n",
            "Compiling objects...\n",
            "Using envvar MAX_JOBS (2) as the number of workers...\n",
            "[1/1] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o.d -I/content/mase-cuda/submodules/cutlass/include -I/content/mase-cuda/submodules/cutlass/tools/util/include -I/tmp/build-env-dw5296y5/lib/python3.11/site-packages/torch/include -I/tmp/build-env-dw5296y5/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/tmp/build-env-dw5296y5/lib/python3.11/site-packages/torch/include/TH -I/tmp/build-env-dw5296y5/lib/python3.11/site-packages/torch/include/THC -I/usr/local/cuda/include -I/tmp/build-env-dw5296y5/include -I/usr/include/python3.11 -c -c /content/mase-cuda/src/csrc/bind.cu -o /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DNDEBUG -std=c++17 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=mase_cuda_ext -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75\n",
            "x86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /content/mase-cuda/build/temp.linux-x86_64-cpython-311/src/csrc/bind.o -L/tmp/build-env-dw5296y5/lib/python3.11/site-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lcuda -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/mase_cuda_ext.cpython-311-x86_64-linux-gnu.so\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/wheel\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/dequantize.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/__init__.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/linear.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/mxint8/quantize.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/mxint8\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/utils.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/__init__.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/constants.py -> build/bdist.linux-x86_64/wheel/./mase_cuda\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda/tools\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda/tools/bin_repr.py -> build/bdist.linux-x86_64/wheel/./mase_cuda/tools\n",
            "creating build/bdist.linux-x86_64/wheel/csrc\n",
            "copying build/lib.linux-x86_64-cpython-311/csrc/bind.cu -> build/bdist.linux-x86_64/wheel/./csrc\n",
            "copying build/lib.linux-x86_64-cpython-311/mase_cuda_ext.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/.\n",
            "running install_egg_info\n",
            "Copying src/mase_cuda.egg-info to build/bdist.linux-x86_64/wheel/./mase_cuda-0.0.1-py3.11.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.linux-x86_64/wheel/mase_cuda-0.0.1.dist-info/WHEEL\n",
            "creating '/content/mase-cuda/dist/.tmp-n7s3ezvp/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'mase_cuda_ext.cpython-311-x86_64-linux-gnu.so'\n",
            "adding 'csrc/bind.cu'\n",
            "adding 'mase_cuda/__init__.py'\n",
            "adding 'mase_cuda/constants.py'\n",
            "adding 'mase_cuda/utils.py'\n",
            "adding 'mase_cuda/mxint8/__init__.py'\n",
            "adding 'mase_cuda/mxint8/dequantize.py'\n",
            "adding 'mase_cuda/mxint8/linear.py'\n",
            "adding 'mase_cuda/mxint8/quantize.py'\n",
            "adding 'mase_cuda/tools/bin_repr.py'\n",
            "adding 'mase_cuda-0.0.1.dist-info/METADATA'\n",
            "adding 'mase_cuda-0.0.1.dist-info/WHEEL'\n",
            "adding 'mase_cuda-0.0.1.dist-info/top_level.txt'\n",
            "adding 'mase_cuda-0.0.1.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[1m\u001b[92mSuccessfully built \u001b[4mmase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl\u001b[0m\u001b[1m\u001b[92m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! TORCH_CUDA_ARCH_LIST=\"7.5\" MAX_JOBS=$(nproc --all) python -m build --wheel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ©¹ We create a new env and run experiments there to avoid errors like `cuda pytorch undefined symbol` raied by the Colab's built-in Python\n",
        "\n",
        "When the wheel is built, install the mase-cuda wheel into **a new dev env**\n",
        "\n",
        "- Open **Colab termimal** and run the following commands to create a dev env and install mase-cuda:\n",
        "\n",
        "  ```bash\n",
        "  cd mase-cuda\n",
        "  tox -e dev # create dev env\n",
        "  . .tox/dev/bin/activate # activate dev env\n",
        "  which pip # ensure this is the pip in .tox/dev\n",
        "  pip install dist/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl # install mase-cuda\n",
        "  ```\n",
        "- Colab terminal: Running the following command to profile dequantization latency (CPU vs GPU). This is slow:\n",
        "\n",
        "  ```bash\n",
        "  pytest -v --log-cli-level INFO test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency\n",
        "  ```\n",
        "\n",
        "The output looks like this\n",
        "\n",
        "```bash\n",
        "============================================================== test session starts ===============================================================\n",
        "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0 -- /content/mase-cuda/.tox/dev/bin/python\n",
        "cachedir: .pytest_cache\n",
        "rootdir: /content/mase-cuda\n",
        "configfile: tox.ini\n",
        "collected 1 item                                                                                                                                 \n",
        "\n",
        "test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency\n",
        "----------------------------------------------------------------- live log call ------------------------------------------------------------------\n",
        "INFO     test_dequantize1d:test_dequantize1d.py:203\n",
        "+-----------+------------+------------------------+------------------------+---------------------+\n",
        "|     m     | group_size |      latency_cpu       |      latency_gpu       |     GPU speedup     |\n",
        "+-----------+------------+------------------------+------------------------+---------------------+\n",
        "|   1024    |     8      | 1.8215179443359376e-05 | 5.830879891291261e-05  | 0.31239160783546144 |\n",
        "|   1024    |     16     | 1.1014938354492188e-05 | 2.411839971318841e-05  | 0.45670270355744236 |\n",
        "|   1024    |     32     | 1.0752677917480469e-05 | 2.3455999884754422e-05 | 0.45841908127179576 |\n",
        "|   1024    |     64     | 1.0418891906738282e-05 | 2.366719990968704e-05  | 0.44022495041645404 |\n",
        "|   1024    |    128     | 1.043081283569336e-05  | 2.370399972423911e-05  | 0.44004442106987857 |\n",
        "|   1024    |    256     | 9.298324584960938e-06  | 2.200479982420802e-05  | 0.42255892619989316 |\n",
        "|   1024    |    512     | 9.393692016601562e-06  | 2.2529599815607072e-05 | 0.41694890692617675 |\n",
        "|  2097152  |     8      |  0.019188427925109865  | 3.4844799526035784e-05 |  550.682689701584   |\n",
        "|  2097152  |     16     |  0.018679165840148927  | 3.453759923577308e-05  |  540.8356762910598  |\n",
        "|  2097152  |     32     |  0.018654394149780273  | 3.327519977465272e-05  |  560.6095313059608  |\n",
        "|  2097152  |     64     |  0.01862926483154297   | 3.344159927219152e-05  |  557.068598302181   |\n",
        "...\n",
        "| 234881024 |     32     |   2.2112363457679747   |  0.008763614416122436  | 252.32013194235924  |\n",
        "| 234881024 |     64     |   2.3624018669128417   |  0.008794196844100953  | 268.63190678947717  |\n",
        "| 234881024 |    128     |   2.402256155014038    |  0.008851744079589843  |  271.3878907268803  |\n",
        "| 234881024 |    256     |   2.4408880949020384   |  0.008949855947494508  |  272.7293164517759  |\n",
        "| 234881024 |    512     |   2.4862973570823668   |  0.004568324756622315  |  544.2470685732645  |\n",
        "+-----------+------------+------------------------+------------------------+---------------------+\n",
        "PASSED                                                                                                                                     [100%]\n",
        "\n",
        "========================================================= 1 passed in 466.97s (0:07:46) ==========================================================\n",
        "```"
      ],
      "metadata": {
        "id": "UbFWFS7BWowY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd mase-cuda\n",
        "!tox -e dev # create dev env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Y__o37IVnn",
        "outputId": "f376070b-afe4-4e62-ed8e-a8d9dc70bd9a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: mase-cuda: No such file or directory\n",
            "\u001b[1m\u001b[35mdev:\u001b[0m\u001b[36m install_deps\u001b[22m\u001b[2m>\u001b[0m python -I -m pip install black build 'colorlog>=6.8.2' ipython ml_dtypes ninja 'numpy>=1.26.4' 'pytest>=8.2.0' 'scipy>=1.12.0' tabulate 'torch>=2.3.0'\u001b[0m\n",
            "\u001b[32m  dev: OK (143.02 seconds)\u001b[39m\n",
            "\u001b[32m  congratulations :) (143.36 seconds)\u001b[39m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!. .tox/dev/bin/activate && which pip && pip install dist/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl && pip install transformers && pytest -v --log-cli-level INFO test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHxdTjK3JIvy",
        "outputId": "6b17bab7-8d59-44ca-b53e-f1c828946136"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mase-cuda/.tox/dev/bin/pip\n",
            "Processing ./dist/mase_cuda-0.0.1-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in ./.tox/dev/lib/python3.11/site-packages (from mase-cuda==0.0.1) (2.2.2)\n",
            "Requirement already satisfied: torch in ./.tox/dev/lib/python3.11/site-packages (from mase-cuda==0.0.1) (2.6.0)\n",
            "Requirement already satisfied: filelock in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (3.1.5)\n",
            "Requirement already satisfied: fsspec in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (2025.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./.tox/dev/lib/python3.11/site-packages (from torch->mase-cuda==0.0.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.tox/dev/lib/python3.11/site-packages (from sympy==1.13.1->torch->mase-cuda==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.tox/dev/lib/python3.11/site-packages (from jinja2->torch->mase-cuda==0.0.1) (3.0.2)\n",
            "Installing collected packages: mase-cuda\n",
            "Successfully installed mase-cuda-0.0.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: filelock in ./.tox/dev/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.tox/dev/lib/python3.11/site-packages (from transformers) (2.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.tox/dev/lib/python3.11/site-packages (from transformers) (24.2)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers)\n",
            "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.tox/dev/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.tox/dev/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
            "Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
            "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 huggingface-hub-0.28.1 idna-3.10 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.2 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.3 urllib3-2.3.0\n",
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0 -- /content/mase-cuda/.tox/dev/bin/python\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/mase-cuda\n",
            "configfile: tox.ini\n",
            "collected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "test/py/mxint8/dequantize/test_dequantize1d.py::test_ext_dequantize1d_latency \n",
            "\u001b[1m------------------------------------------ live log call -------------------------------------------\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m test_dequantize1d:test_dequantize1d.py:203 \n",
            "+-----------+------------+------------------------+------------------------+---------------------+\n",
            "|     m     | group_size |      latency_cpu       |      latency_gpu       |     GPU speedup     |\n",
            "+-----------+------------+------------------------+------------------------+---------------------+\n",
            "|   1024    |     8      | 1.6355514526367186e-05 | 9.208320006728173e-05  | 0.17761670439794475 |\n",
            "|   1024    |     16     | 1.0073184967041016e-05 | 2.1960000135004522e-05 | 0.45870605214542914 |\n",
            "|   1024    |     32     | 9.608268737792968e-06  | 2.1776000224053858e-05 | 0.4412320278716582  |\n",
            "|   1024    |     64     | 8.451938629150391e-06  | 2.191680008545518e-05  | 0.38563743777356524 |\n",
            "|   1024    |    128     | 8.678436279296875e-06  | 2.2073600068688394e-05 | 0.39315907927530663 |\n",
            "|   1024    |    256     |   8.392333984375e-06   | 2.1001600101590156e-05 | 0.3996045036463468  |\n",
            "|   1024    |    512     | 8.857250213623047e-06  | 2.068160017952323e-05  | 0.4282671619574473  |\n",
            "|  2097152  |     8      |  0.018153321743011475  | 0.0001589648000895977  |  114.1971161715026  |\n",
            "|  2097152  |     16     |  0.01761068105697632   | 0.00015844959914684294 | 111.14374003973116  |\n",
            "|  2097152  |     32     |  0.018003463745117188  | 0.00015778399780392647 |  114.1019621488458  |\n",
            "|  2097152  |     64     |  0.017447733879089357  | 0.00015757919922471046 | 110.72358512374852  |\n",
            "|  2097152  |    128     |  0.018094611167907716  | 0.00015756319761276244 | 114.84033988938336  |\n",
            "|  2097152  |    256     |  0.01746176481246948   | 0.00015759039968252182 | 110.80474983024075  |\n",
            "|  2097152  |    512     |  0.01788308620452881   | 0.0001589183986186981  | 112.52999249908571  |\n",
            "| 16777216  |     8      |  0.17041447162628173   | 0.0017218592047691347  |  98.97119994147882  |\n",
            "| 16777216  |     16     |   0.2032654643058777   | 0.0016178687989711763  | 125.63779240636622  |\n",
            "| 16777216  |     32     |  0.16679929494857787   | 0.0016226144015789032  | 102.79663164968333  |\n",
            "| 16777216  |     64     |  0.15906516313552857   | 0.0012041312098503112  |  132.0995268906803  |\n",
            "| 16777216  |    128     |  0.17150002717971802   | 0.0012042864084243774  |  142.4080069159788  |\n",
            "| 16777216  |    256     |  0.16935877799987792   | 0.0012060848057270051  | 140.42028984669253  |\n",
            "| 16777216  |    512     |  0.15809448957443237   | 0.0012042863965034485  | 131.27648874341466  |\n",
            "| 58720256  |     8      |   0.6109309196472168   |  0.005648593592643737  | 108.15628875174218  |\n",
            "| 58720256  |     16     |   0.6247500538825989   |  0.005665102410316467  | 110.28045190231586  |\n",
            "| 58720256  |     32     |   0.6212068915367126   | 0.0056850495576858525  | 109.27026848813964  |\n",
            "| 58720256  |     64     |   0.6518872737884521   |  0.005656451153755188  | 115.24669020710611  |\n",
            "| 58720256  |    128     |   0.583406388759613    |  0.004196457624435425  | 139.02353865377165  |\n",
            "| 58720256  |    256     |   0.5850659012794495   |  0.004207280039787292  | 139.06036578183864  |\n",
            "| 58720256  |    512     |   0.5793596625328064   |  0.004199860835075378  | 137.94734761072343  |\n",
            "| 234881024 |     8      |   2.3942604184150698   |  0.017538492679595948  | 136.51460602429768  |\n",
            "| 234881024 |     16     |   2.4724790453910828   |  0.018563184070587158  | 133.19261587825636  |\n",
            "| 234881024 |     32     |   2.438396763801575    |  0.01866573758125305   | 130.63490007759356  |\n",
            "| 234881024 |     64     |   2.5425369262695314   |  0.018299332761764524  | 138.94151001953614  |\n",
            "| 234881024 |    128     |   2.5408451318740846   |  0.018201966285705567  | 139.59179420464426  |\n",
            "| 234881024 |    256     |   2.6608150601387024   |  0.01843252806663513   |  144.354320282038   |\n",
            "| 234881024 |    512     |   2.433074617385864    |  0.014220660734176636  | 171.09434384707845  |\n",
            "+-----------+------------+------------------------+------------------------+---------------------+\n",
            "\u001b[32mPASSED\u001b[0m\u001b[32m                                                                                       [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 497.89s (0:08:17)\u001b[0m\u001b[32m ===================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90MO9ajpz54_"
      },
      "source": [
        "### FP32 Deberta Demo\n",
        "\n",
        "- Colab Terminal: Install transformers in the dev env: `pip install transformers`\n",
        "\n",
        "- demo.py: Copy the following codes into a new file demo.py\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"AnkitAI/deberta-xlarge-base-emotions-classifier\"\n",
        "# if you meet OOM error, try this smaller model, but the quantization effect may not be obvious later\n",
        "# model_name = \"AnkitAI/deberta-v3-small-base-emotions-classifier\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).cuda().eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label2emotion = {idx: emotion for emotion, idx in model.config.label2id.items()}\n",
        "\n",
        "# Example usage\n",
        "@torch.no_grad()\n",
        "def predict_emotion(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = logits.argmax(dim=1)\n",
        "    predictions = label2emotion[predictions.item()]\n",
        "    top3_values, top3_indices = torch.topk(logits, 3)\n",
        "    top3_values = top3_values.cpu().tolist()\n",
        "    top3_indices = top3_indices.cpu().tolist()\n",
        "    return predictions, (top3_values, top3_indices)\n",
        "\n",
        "\n",
        "text = \"I'm so happy with the results!\"\n",
        "emotion, top3 = predict_emotion(model, tokenizer, text)\n",
        "\n",
        "print(\"Index to Emotion Mapping:\", label2emotion)\n",
        "print(\"Input text:\", text)\n",
        "print(\"Detected Emotion:\", emotion)\n",
        "print(f\"top3 logits: {top3[0]}, top3 indices: {top3[1]}\")\n",
        "```\n",
        "\n",
        "- Colab Terminal: run demo.py in the dev env\n",
        "\n",
        "The output looks like this:\n",
        "\n",
        "```bash\n",
        "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.12k/1.12k [00:00<00:00, 7.73MB/s]\n",
        "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.04G/3.04G [01:12<00:00, 42.0MB/s]\n",
        "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31k/1.31k [00:00<00:00, 12.6MB/s]\n",
        "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 798k/798k [00:00<00:00, 5.79MB/s]\n",
        "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 3.35MB/s]\n",
        "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.11M/2.11M [00:00<00:00, 10.3MB/s]\n",
        "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 969/969 [00:00<00:00, 8.67MB/s]\n",
        "Index to Emotion Mapping: {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
        "Input text: I'm so happy with the results!\n",
        "Detected Emotion: joy\n",
        "top3 logits: [[7.345228672027588, -1.4850201606750488, -1.6403964757919312]], top3 indices: [[1, 4, 0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!. .tox/dev/bin/activate && python ../demo.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD-B5WZhJhdl",
        "outputId": "5b77dc46-28f3-425d-a397-9ea3d8c8d8ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json: 100% 1.12k/1.12k [00:00<00:00, 5.19MB/s]\n",
            "model.safetensors: 100% 3.04G/3.04G [01:12<00:00, 42.1MB/s]\n",
            "tokenizer_config.json: 100% 1.31k/1.31k [00:00<00:00, 9.57MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 3.73MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 2.12MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 9.29MB/s]\n",
            "special_tokens_map.json: 100% 969/969 [00:00<00:00, 7.43MB/s]\n",
            "Index to Emotion Mapping: {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
            "Input text: I'm so happy with the results!\n",
            "Detected Emotion: joy\n",
            "top3 logits: [[7.3452301025390625, -1.4850208759307861, -1.6403967142105103]], top3 indices: [[1, 4, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XrSk9bfz_nZ"
      },
      "source": [
        "### MXINT8 Deberta\n",
        "- demo-q.py: Copy the following codes into a new file demo-q.py. This file creates the quantized Deberta and compare GPU memory usage of MXINT8 model with FP32 model\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import time\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "from mase_cuda.mxint8.linear import QLinearPacked\n",
        "\n",
        "init_memory = torch.cuda.memory_allocated()  # in bytes\n",
        "model_name = \"AnkitAI/deberta-xlarge-base-emotions-classifier\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=torch.float32).cuda().eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label2emotion = {idx: emotion for emotion, idx in model.config.label2id.items()}\n",
        "\n",
        "mxint8_group_size = 32\n",
        "assert model.config.hidden_size % mxint8_group_size == 0\n",
        "assert model.config.intermediate_size % mxint8_group_size == 0\n",
        "\n",
        "text = \"I'm so happy with the results!\"\n",
        "\n",
        "def timed_gpu(fn):\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    start.record()\n",
        "    result1, result2,  = fn()\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return result1, result2, start.elapsed_time(end) / 1000\n",
        "\n",
        "def time_model(fn, model, tokenizer,  n=500):\n",
        "    times = []\n",
        "    sentences = [\n",
        "    \"I love this product! It works perfectly and exceeded my expectations.\",  # Positive\n",
        "    \"This service is awful; Iâ€™ll never use it again.\",  # Negative\n",
        "    \"The package arrived today at 3:00 PM.\",  # Neutral\n",
        "    \"The sunset today was absolutely breathtaking!\",  # Positive\n",
        "    \"Iâ€™m so frustrated with the constant delays and poor communication.\",  # Negative\n",
        "    \"Great, another meeting that couldâ€™ve been an email.\",  # Sarcastic/Passive-Aggressive\n",
        "    \"I cant believe it, thats soo bad!\",\n",
        "    \"Interesting choice of design... not sure how I feel about it.\",  # Ambiguous\n",
        "    \"This is the worst experience Iâ€™ve ever hadâ€”terrible customer support!\",  # Strongly Negative\n",
        "    \"I'm so happy with the results!\"\n",
        "]\n",
        "    result = [0,0,0,0,0,0,0,0,0,0]\n",
        "    for i in range(n):\n",
        "        result1, result2, t = timed_gpu(lambda: fn(model, tokenizer, sentences[i%10]))\n",
        "        times.append(t)\n",
        "        result[i%10] = result1\n",
        "    avg_time = sum(times) / len(times)\n",
        "    print(result)\n",
        "    return result1, result2, avg_time\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_emotion(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = logits.argmax(dim=1)\n",
        "    predictions = label2emotion[predictions.item()]\n",
        "    top3_values, top3_indices = torch.topk(logits, 3)\n",
        "    top3_values = top3_values.cpu().tolist()\n",
        "    top3_indices = top3_indices.cpu().tolist()\n",
        "    return predictions, (top3_values, top3_indices)\n",
        "\n",
        "# check the GPU memory usage of FP32 model\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "emotion_fp32, top3_fp32, time_fp32 = time_model(predict_emotion, model, tokenizer)\n",
        "peak_memory_fp32 = torch.cuda.max_memory_allocated() - init_memory  # in bytes\n",
        "\n",
        "\n",
        "def set_layer_by_name(module: torch.nn.Module, name: str, new_layer: torch.nn.Module):\n",
        "    levels = name.split(\".\")\n",
        "    if len(levels) > 1:\n",
        "        mod_ = module\n",
        "        for l_idx in range(len(levels) - 1):\n",
        "            if levels[l_idx].isdigit():\n",
        "                mod_ = mod_[int(levels[l_idx])]\n",
        "            else:\n",
        "                mod_ = getattr(mod_, levels[l_idx])\n",
        "        setattr(mod_, levels[-1], new_layer)\n",
        "    else:\n",
        "        setattr(module, name, new_layer)\n",
        "\n",
        "\n",
        "for layer_name, layer in model.named_modules():\n",
        "    if not isinstance(layer, torch.nn.Linear):\n",
        "        continue\n",
        "    if \"classifier\" in layer_name:\n",
        "        continue\n",
        "    layer.cuda()\n",
        "    layer_q = QLinearPacked.build_from_linear(layer, group_size=mxint8_group_size)\n",
        "    set_layer_by_name(model, layer_name, layer_q)\n",
        "    del layer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "emotion_mxint8, top3_mxint8, time_mxint8 = time_model(predict_emotion, model, tokenizer)\n",
        "peak_memory_mxint8 = torch.cuda.max_memory_allocated() - init_memory  # in bytes\n",
        "\n",
        "print(f\"FP32 model peak memory: {peak_memory_fp32/1024**2:.4f} MB\")\n",
        "print(f\"FP32 single inference time: {time_fp32:.4f} s\")\n",
        "print(f\"PF32 prediction: {emotion_fp32}\")\n",
        "print(f\"FP32 top3 logits: {top3_fp32[0]}, indices: {top3_fp32[1]}\")\n",
        "\n",
        "print(f\"MXINT8 model peak memory: {peak_memory_mxint8/1024**2:.4f} MB\")\n",
        "print(f\"MXINT8 single inference time: {time_mxint8:.4f} s\")\n",
        "print(f\"MXINT8 prediction: {emotion_mxint8}\")\n",
        "print(f\"MXINT8 top3 logits: {top3_mxint8[0]}, indices: {top3_mxint8[1]}\")\n",
        "```\n",
        "\n",
        "- Colab Terminal: Run demo-q.py in the dev env\n",
        "\n",
        "The output looks like this:\n",
        "\n",
        "```bash\n",
        "FP32 model peak memory: 2906.1997 MB\n",
        "PF32 prediction: joy\n",
        "FP32 top3 logits: [[7.345228672027588, -1.4850201606750488, -1.6403964757919312]], indices: [[1, 4, 0]]\n",
        "MXINT8 model peak memory: 976.1616 MB\n",
        "MXINT8 prediction: joy\n",
        "MXINT8 top3 logits: [[7.350157737731934, -1.488325834274292, -1.649757981300354]], indices: [[1, 4, 0]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!. .tox/dev/bin/activate && python ../demo-q.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khps-Qp-KLwM",
        "outputId": "a928d2db-970a-4c64-e63f-225fa01a5c67"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['joy', 'anger', 'joy', 'joy', 'anger', 'joy', 'sadness', 'fear', 'anger', 'joy']\n",
            "['joy', 'anger', 'joy', 'joy', 'anger', 'joy', 'sadness', 'fear', 'anger', 'joy']\n",
            "FP32 model peak memory: 2908.2578 MB\n",
            "FP32 single inference time: 0.1017 s\n",
            "PF32 prediction: joy\n",
            "FP32 top3 logits: [[7.3452301025390625, -1.4850208759307861, -1.6403967142105103]], indices: [[1, 4, 0]]\n",
            "MXINT8 model peak memory: 978.0288 MB\n",
            "MXINT8 single inference time: 0.1342 s\n",
            "MXINT8 prediction: joy\n",
            "MXINT8 top3 logits: [[7.350157737731934, -1.4883261919021606, -1.6497581005096436]], indices: [[1, 4, 0]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "0XrSk9bfz_nZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}